{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the KNN algorithm work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a simple case to understand this algorithm. Following is a spread of red circles (RC) and green squares (GS) : <br>\n",
    "![DataFrame_structure](img/scenario1.png) <br>\n",
    "\n",
    "You intend to find out the class of the blue star (BS) . BS can either be RC or GS and nothing else. The “K” is KNN algorithm is the nearest neighbors we wish to take vote from. Let’s say K = 3. Hence, we will now make a circle with BS as center just as big as to enclose only three datapoints on the plane. Refer to following diagram for more details:\n",
    "<br>\n",
    "![DataFrame_structure](img/scenario2.png)\n",
    "<br>\n",
    "The three closest points to BS is all RC. Hence, with good confidence level we can say that the BS should belong to the class RC. Here, the choice became very obvious as all three votes from the closest neighbor went to RC. The choice of the parameter K is very crucial in this algorithm. Next we will understand what are the factors to be considered to conclude the best K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Psudocode of KNN :\n",
    "\n",
    "    1. Load the data \n",
    "    2. Initialise the value of k \n",
    "    3. For getting the predicted class, iterate from 1 to total number of training data points \n",
    "        1. Calculate the distance between test data and each row of training data. Here we will use Euclidean distance as our distance metric since it’s the most popular method. The other metrics that can be used are Chebyshev, cosine, etc. \n",
    "        2. Sort the calculated distances in ascending order based on distance values \n",
    "        3. Get top k rows from the sorted array \n",
    "        4. Get the most frequent class of these rows \n",
    "        5. Return the predicted class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714285714285714\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, cross_validation, neighbors\n",
    "\n",
    "df = pd.read_csv('C:/Users/Putra/Desktop/Data Science/breast-cancer-wisconsin.data.csv', delimiter = ',')\n",
    "df.replace('?', -99999, inplace = True) #membuat NaN value sebagai outliear\n",
    "df.drop(['id'], 1, inplace = True)\n",
    "\n",
    "X = np.array(df.drop(['class'],1))\n",
    "y = np.array(df['class'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(X,y, test_size = 0.2)\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier() #fungsi K Neighbors Classifier\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "#Algorithm Accuracy\n",
    "accuracy = clf.score(x_test,y_test)\n",
    "print(accuracy)\n",
    "\n",
    "#Prediction\n",
    "example_measures = np.array([[4,2,1,1,1,2,3,2,1]])\n",
    "example_measures = example_measures.reshape(len(example_measures),-1) #reshape data, untuk k sampel di sklearn\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro and Cons about KNN algorithm <br>\n",
    "Pro : <br>\n",
    "\n",
    " 1. K-NN is pretty intuitive and simple: K-NN algorithm is very simple to understand and equally easy to implement. To classify the new data point K-NN algorithm reads through whole dataset to find out K nearest neighbors.\n",
    " 2. K-NN has no assumptions: K-NN is a non-parametric algorithm which means there are assumptions to be met to implement K-NN. Parametric models like linear regression has lots of assumptions to be met by data before it can be implemented which is not the case with K-NN.\n",
    " 3. No Training Step: K-NN does not explicitly build any model, it simply tags the new data entry based learning from historical data. New data entry would be tagged with majority class in the nearest neighbor.\n",
    " 4. It constantly evolves: Given it’s an instance-based learning; k-NN is a memory-based approach. The classifier immediately adapts as we collect new training data. It allows the algorithm to respond quickly to changes in the input during real-time use.\n",
    " 5. Very easy to implement for multi-class problem: Most of the classifier algorithms are easy to implement for binary problems and needs effort to implement for multi class whereas K-NN adjust to multi class without any extra efforts.\n",
    " 6. Can be used both for Classification and Regression: One of the biggest advantages of K-NN is that K-NN can be used both for classification and regression problems.\n",
    " 7. One Hyper Parameter: K-NN might take some time while selecting the first hyper parameter but after that rest of the parameters are aligned to it.\n",
    " 8. Variety of distance criteria to be choose from: K-NN algorithm gives user the flexibility to choose distance while building K-NN model.\n",
    "        Euclidean Distance\n",
    "        Hamming Distance\n",
    "        Manhattan Distance\n",
    "        Minkowski Distance\n",
    "\n",
    "Cons : <br>\n",
    "\n",
    " 1.  K-NN slow algorithm: K-NN might be very easy to implement but as dataset grows efficiency or speed of algorithm declines very fast.\n",
    " 2.   Curse of Dimensionality: KNN works well with small number of input variables but as the numbers of variables grow K-NN algorithm struggles to predict the output of new data point.\n",
    " 3.   K-NN needs homogeneous features: If you decide to build k-NN using a common distance, like Euclidean or Manhattan distances, it is completely necessary that features have the same scale, since absolute differences in features weight the same, i.e., a given distance in feature 1 must means the same for feature 2.\n",
    " 4.   Optimal number of neighbors: One of the biggest issues with K-NN is to choose the optimal number of neighbors to be consider while classifying the new data entry.\n",
    " 5.  Imbalanced data causes problems: k-NN doesn’t perform well on imbalanced data. If we consider two classes, A and B, and the majority of the training data is labeled as A, then the model will ultimately give a lot of preference to A. This might result in getting the less common class B wrongly classified.\n",
    " 6.   Outlier sensitivity: K-NN algorithm is very sensitive to outliers as it simply chose the neighbors based on distance criteria.\n",
    " 7.   Missing Value treatment: K-NN inherently has no capability of dealing with missing value problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
